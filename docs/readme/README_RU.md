![–õ–æ–≥–æ—Ç–∏–ø](https://github.com/Solrikk/PicTrace/blob/main/assets/OpenCV%20-%20result/bee.jpg)

<div align="center">
  <h3>
    <a href="https://github.com/Solrikk/PicTrace/blob/main/README.md">–ê–Ω–≥–ª–∏–π—Å–∫–∏–π</a> |
    <a href="https://github.com/Solrikk/PicTrace/blob/main/docs/readme/README_RU.md">‚ú¶ –†—É—Å—Å–∫–∏–π ‚ú¶</a> |
    <a href="https://github.com/Solrikk/PicTrace/blob/main/docs/readme/README_GE.md">–ù–µ–º–µ—Ü–∫–∏–π</a> |
    <a href="https://github.com/Solrikk/PicTrace/blob/main/docs/readme//README_JP.md">–Ø–ø–æ–Ω—Å–∫–∏–π</a> |
    <a href="https://github.com/Solrikk/PicTrace/blob/main/docs/readme/README_KR.md">–ö–æ—Ä–µ–π—Å–∫–∏–π</a> |
    <a href="https://github.com/Solrikk/PicTrace/blob/main/docs/readme/README_CN.md">–ö–∏—Ç–∞–π—Å–∫–∏–π</a>
  </h3>
</div>

-----------------

# PicTrace üîç

‚ú® **PicTrace** - —ç—Ç–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ **Python**, –æ—Å–Ω–∞—â–µ–Ω–Ω–æ–µ **–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º (GUI)** –∏ **–≤–µ–±-–≤–µ—Ä—Å–∏–µ–π –Ω–∞ FastAPI**, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å **–≤–∏–∑—É–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è** –∏–∑ –æ–±—à–∏—Ä–Ω–æ–≥–æ **—Ñ–æ—Ç–æ–∞—Ä—Ö–∏–≤–∞**. –ò—Å–ø–æ–ª—å–∑—É—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ **–≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è** –∏ **—Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**, **PicTrace** –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç **–±—ã—Å—Ç—Ä—ã–µ –∏ —Ç–æ—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞**, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –∏–¥–µ–∞–ª—å–Ω—ã–º –¥–ª—è –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ **–∫–∞—Ç–∞–ª–æ–≥–∏–∑–∞—Ü–∏—è**, **–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è** –∏ **–∞–Ω–∞–ª–∏–∑ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö**.

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏:

–•–æ—Ç–∏—Ç–µ —É–≤–∏–¥–µ—Ç—å, –∫–∞–∫ _PicTrace_ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏? 

**–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏ —É–±–µ–¥–∏—Ç–µ—Å—å —Å–∞–º–∏!**

https://pictrace.replit.app/

![–î–µ–º–æ PicTrace](https://github.com/Solrikk/PicTrace/blob/main/assets/gif/Pictrace.gif)

## –ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã —Å PicTrace:
| **–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞** | **–ö–æ–º–∞–Ω–¥—ã –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –∑–∞–ø—É—Å–∫–∞** |
|--------------------------|-----------------------------------|
| üêß**Linux**              | ```bash sudo git clone https://github.com/Solrikk/PicTrace.git cd PicTrace sudo pip install poetry sudo poetry install sudo poetry run python3 main.py ``` |
| üçé**macOS**              | ```bash sudo git clone https://github.com/Solrikk/PicTrace.git cd PicTrace sudo pip install poetry sudo poetry install sudo poetry run python3 main.py ``` |
| ü™ü**Windows**            | –û—Ç–∫—Ä–æ–π—Ç–µ –∫–æ–º–∞–Ω–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É –æ—Ç –∏–º–µ–Ω–∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: ```bash git clone https://github.com/Solrikk/PicTrace.git cd PicTrace pip install poetry poetry install poetry run python main.py ``` |

![–î–µ–º–æ PicTrace](https://github.com/Solrikk/PicTrace/blob/main/assets/gif/shell-PicTrrace.gif)

-----------------

## –§—É–Ω–∫—Ü–∏–∏ ‚ö°

- **_–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π_**
    - **Tkinter**: –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–¥–æ–±–Ω—ã–π –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å PicTrace. [–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏](https://docs.python.org/3/library/tkinter.html)
    - **TensorFlow –∏ Keras**: –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ ResNet50 –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. [–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏](https://www.tensorflow.org/api_docs/python/tf/keras)
    - **numpy**: –ú–Ω–æ–≥–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–º–∏ –º–∞—Å—Å–∏–≤–∞–º–∏, –æ–±–ª–µ–≥—á–∞—é—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö. [–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏](https://numpy.org/doc/)
    - **Pillow (PIL)**: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏, –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. [–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏](https://pillow.readthedocs.io/en/stable/)
    - **pickle**: –ú–æ–¥—É–ª—å –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ Python, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∑–∞—Ä–∞–Ω–µ–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. [–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏](https://docs.python.org/3/library/pickle.html)
    - **hashlib**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ö–µ—à–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–ø—Ä–∞–≤–ª—è—Ç—å –∫–∞–∂–¥—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º. [–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏](https://docs.python.org/3/library/hashlib.html)
    - **scikit-image**: –ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ —Ñ—É–Ω–∫—Ü–∏—è `structural_similarity (SSIM)` –∏–∑ —ç—Ç–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ–≤—ã—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. [–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏](https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.structural_similarity)
    - **OpenCV (cv2)**: –ù–∞–¥–µ–∂–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è –∑–∞–≥—Ä—É–∑–∫—É, –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. [–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏](https://docs.opencv.org/master/)
    - **zipfile**: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç ZIP-–∞—Ä—Ö–∏–≤—ã —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –æ–±–ª–µ–≥—á–∞—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. [–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏](https://docs.python.org/3/library/zipfile.html)

-----------------

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: 
_–î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –¥–µ—Ç–∞–ª–µ–π –∏ –≤–æ–∑–º–æ–∂–Ω—ã–º –Ω–∞–ª–∏—á–∏–µ–º —à—É–º–æ–≤ –∏–ª–∏ –∏—Å–∫–∞–∂–µ–Ω–∏–π –¥–∞–∂–µ —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ **20%** –∏ –≤—ã—à–µ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –í —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö –Ω–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–∂–∏–¥–∞–µ–º –∏–∑-–∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º–∞._
|–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 1 vs –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 2|–°—Ö–æ–∂–µ—Å—Ç—å|–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ|
|:-:|:-:|:-:|
|<img src="https://github.com/Solrikk/PicTrace/blob/main/assets/result/images/result_3-1.png" alt="" width="500"/>|**27,12%**|<img src="https://github.com/Solrikk/PicTrace/blob/main/assets/result/images/palegleam.jpg" alt="" width="300"/>|
|<img src="https://github.com/Solrikk/PicTrace/blob/main/assets/result/images/result_2.png" alt="" width="500"/>|**25,44%**|<img src="https://github.com/Solrikk/PicTrace/blob/main/assets/result/images/ryan-yao.jpg" alt="" width="300"/>|
|<img src="https://github.com/Solrikk/PicTrace/blob/main/assets/result/images/result_3.png" alt="" width="500"/>|**44,16%**|<img src="https://github.com/Solrikk/PicTrace/blob/main/assets/result/images/taro-ohtani.jpg" alt="" width="300"/>|

## –ü—Ä–∏–º–µ—Ä—ã: 
(**_–∫–æ–¥ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏_**)

```Python 
async def find_similar_images(file_path):
    # Load the data from the database, which contains information about images.
    db_data = load_db()
    # Read the target image from the given file path.
    target_image = cv2.imread(file_path)
    # Extract features from the target image using a pre-trained model.
    target_features = extract_features(target_image)
    # Create an aiohttp asynchronous session for handling HTTP requests.
    async with aiohttp.ClientSession() as session:
        # Create asynchronous tasks for the compare_images function for each image in the database.
        tasks = [
            compare_images(session, entry, target_features) for entry in db_data
            if "url" in entry  # Only perform comparisons for entries that contain an image URL.
        ]
        # Wait for all tasks to complete and gather the results.
        results = await asyncio.gather(*tasks)
    # Filter the results, keeping only those with a similarity score greater than 0.
    valid_results = filter(lambda x: x[0] > 0, results)   
    # Sort the filtered results by similarity score in descending order and take the top 5.
    sorted_results = sorted(valid_results, key=lambda x: x[0], reverse=True)[:5]
    # Create a list to store the URLs of the similar images.
    similar_images = []
    for result in sorted_results:
        if result[1]:
            similar_images.append(result[1])
    # Return the list of URLs of the similar images.
    return similar_images
```

-----------------

## _OpenCV (Open Source Computer Vision Library):_

<img src="https://github.com/Solrikk/PicTrace/blob/main/assets/OpenCV%20-%20result/parrot.png" width="95%" /> 

**OpenCV** is a powerful computer vision library that provides tools for image and video processing. It is widely used in fields related to machine vision, image recognition, video analysis, and more. The library includes a wide range of algorithms for image analysis, such as object detection, face recognition, motion tracking, video manipulation, and more.

Key features of **OpenCV** include:
1. **Loading and saving images üñºÔ∏è**: Supports various image formats and allows for easy loading, resizing, and saving of images, which is crucial for handling large datasets.
2. **Image processing ‚ú®**: Provides functions for filtering images, converting them to grayscale, resizing, rotating, and other manipulations. This is important for preprocessing images before analysis.
3. **Object detection üîç**: Includes algorithms for detecting edges, corners, and other key points, which helps in identifying and tracking specific objects in a frame.
4. **Object recognition üëÅÔ∏è**: Offers tools for recognizing faces, gestures, and other objects in images and videos, which is key for many computer vision applications.

## Neural Network Model (ResNet50):

![image](https://github.com/Solrikk/PicTrace/assets/70236693/d47bd022-8a05-48fc-b6c8-147ec99520ce)


The **ResNet50** (Residual Network) model is one of the most popular and powerful deep learning architectures for image classification and feature extraction tasks. Your neural network model **ResNet50** provides the following advantages:

1. **Deep residual networks**: Uses residual networks to ease the training of deep neural networks, allowing for the construction of very deep architectures without the risk of vanishing gradients.
2. **Pre-trained weights**: The model comes with pre-trained weights on the ImageNet dataset, which can significantly speed up training and improve accuracy in image classification tasks.
3. **Feature extraction**: The model can be used to extract features from images, which is useful for tasks related to cognitive data analysis and machine learning.
4. **Flexibility**: The model can be used for both classification and the task of extracting and comparing image features, which is suitable for your application.

Together, **OpenCV** and **ResNet50** can be used to create powerful computer vision applications that can analyze visual data and perform complex tasks, such as automatic object recognition and image classification.

---

The ORB method, used in computer vision, is particularly popular for tasks related to object recognition, image matching, and tracking. This method focuses on quickly finding key points on images and describing them in a way that allows for efficient comparison.

<img src="https://github.com/Solrikk/PicTrace/blob/main/assets/ORB/images/ORB3.png" width="65%" /> 

1. **Oriented FAST (Features from Accelerated Segment Test) üöÄ:** This component is responsible for detecting points of interest (or key points) on the image. It quickly identifies corners or edges that stand out in comparison to their surrounding areas. This way, significant or unique sections of the image can be identified.

2. **Rotated BRIEF (Binary Robust Independent Elementary Features) üîÑ:** After key points have been found, it is necessary to create a description for each to allow comparison with key points from another image. BRIEF generates a brief binary description of the points but lacks resistance to image rotation. This is where the "rotated" part comes in - ORB adds the ability to stably describe points even when images are rotated.

Combining these two approaches, ORB provides a fast and efficient way of matching images despite changes in viewing angle, scale, or lighting.

PicTrace uses both **SSIM** and **ORB** methods to find images that are similar to an uploaded image. Here's a simplified explanation of how each method works in the context of your application and contributes to finding similar images:

## How SSIM Works in PicTrace:
1. **Resizing Images üîß:** When comparing the uploaded image to each image in the database, both images are resized to the same dimensions (256x256 pixels). This standardizes the comparison, making it fair and more efficient.
2. **Converting to Grayscale üåë:** Both images are converted to grayscale. This simplifies the comparison by focusing on the structure and intensity of light rather than getting distracted by color differences.
3. **Structural Similarity Comparison üß©:** The SSIM method then compares these grayscale images to assess their structural similarity. A high score means the images are structurally similar.

## How ORB Works in PicTrace:
1. **Detecting Key Points üìç:** ORB first identifies key points in both the uploaded image and each database image. These points are easily recognizable and can be compared between images.
2. **Describing Key Points üñäÔ∏è:** For each detected key point, ORB generates a unique descriptor that summarizes the key point's characteristics. This descriptor is invariant to image rotations.
3. **Matching Key Points üîó:** The application matches key points between the uploaded image and each database image. The process involves finding key points in the database image that have descriptors similar to those of the uploaded image.
4. **Scoring Matches üèÖ:** The more key points that match between two images, the higher the similarity score based on ORB. This score reflects how many distinctive features the images share.

Together, the **SSIM** and **ORB** methods provide a robust and accurate way to find and compare images that are similar to the uploaded image.
